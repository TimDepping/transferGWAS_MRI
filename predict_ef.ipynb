{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import models, transforms"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Resnet + Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dhc/home/laurin.siefermann/conda3/envs/torch_update/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/dhc/home/laurin.siefermann/conda3/envs/torch_update/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Define head structure\n",
    "class Lambda(nn.Module):\n",
    "    def __init__(self, func):\n",
    "        super().__init__()\n",
    "        self.func = func\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.func(x)\n",
    "\n",
    "class FlattenLayer(Lambda):\n",
    "    def __init__(self):\n",
    "        super().__init__(func=lambda x: x.view(len(x), -1))\n",
    "\n",
    "head = nn.Sequential(nn.AdaptiveAvgPool2d((1, 1)), FlattenLayer(), nn.Linear(2048, 1))\n",
    "\n",
    "## Create hook for feature maps of resnet\n",
    "feature_maps = []\n",
    "def hook_fn(module, input, output):\n",
    "    # Store the output of the module in a list\n",
    "    feature_maps.append(output)\n",
    "\n",
    "## Load model \n",
    "model_path = '/dhc/groups/mpws2022cl1/models/testModel_head.pt'\n",
    "m_func = models.resnet50\n",
    "model = m_func(pretrained=True)\n",
    "model.conv1 = nn.Conv2d(\n",
    "                50, 64, kernel_size=7, stride=2, padding=3, bias=False\n",
    "            )\n",
    "checkpoint = torch.load(model_path)\n",
    "model.load_state_dict(checkpoint['resnet_state_dict'])\n",
    "head.load_state_dict(checkpoint['head_state_dict'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load input data \n",
    "mcTensor = torch.load(\"/dhc/groups/mpws2022cl1/tensor/3000_GRAY/1000148_CINE_segmented_LAX_4Ch_mc50.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Transform each img of multi tensor\n",
    "\n",
    "def percentile_scaling_array(\n",
    "        array, lower_percentile=0, upper_percentile=98, min_val=0, max_val=1\n",
    "    ):\n",
    "        lower_bound = np.percentile(array, lower_percentile)\n",
    "        upper_bound = np.percentile(array, upper_percentile)\n",
    "        array = (array - lower_bound) / (upper_bound - lower_bound)\n",
    "        array = array * (max_val - min_val) + min_val\n",
    "        tensor = transforms.ToTensor()(array).float()\n",
    "        return tensor\n",
    "\n",
    "# ToTensor requires a numpy.ndarray (H x W x C)\n",
    "## 1) transform mcTensor to npArrayList (50x 1x1x224x224)\n",
    "npArrays = mcTensor.numpy()\n",
    "npArrayList = np.split(npArrays, 50, axis=0)\n",
    "## 2) remove dimensions of size 1 infront of the array 1,1,224,244 -> 224,244\n",
    "npArrayList = np.squeeze(npArrayList)\n",
    "## 3) add dimentions of size 1 at the end of the array 224,244 -> 224,244,1\n",
    "npArrayList = [array[:, :, np.newaxis] for array in npArrayList]\n",
    "## 4) scale tensor list + toTensor\n",
    "scaledTensorList = [\n",
    "    percentile_scaling_array(array, 0, 98, 0, 1) for array in npArrayList\n",
    "]\n",
    "## 5) Get rid of the first dimension: [1,224,224] -> [224,224]\n",
    "squeezed_tensors = [tensor.squeeze(0) for tensor in scaledTensorList]\n",
    "## 6) Stack all the transformed images back together to a create a 50 channel tensor\n",
    "stacked_tensor = torch.stack(squeezed_tensors, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 50, 224, 224])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Add batch dimension for resnet\n",
    "input_tensor = stacked_tensor.unsqueeze(0) \n",
    "input_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.hooks.RemovableHandle at 0x7f94782e9f60>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the layer 4 module and register the hook\n",
    "layer4 = model.layer4\n",
    "feature_maps = []\n",
    "layer4.register_forward_hook(hook_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass the input tensor through the model\n",
    "model.eval()\n",
    "head.eval()\n",
    "with torch.no_grad():\n",
    "    output = model(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer4_feature_maps = feature_maps[0]\n",
    "layer4_feature_maps.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Feed layer4_feature_maps into head\n",
    "with torch.no_grad():\n",
    "    output = head(layer4_feature_maps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2998]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Ef prediction\n",
    "output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transfer_gwas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dc910ad0ce2a7603f15909527666fa83ce4a8e28f042ea6b6e566ba6f7a130b5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
